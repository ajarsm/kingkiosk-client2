import 'dart:async';
import 'dart:convert';
import 'dart:math' as math;
import 'dart:typed_data';
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:flutter_webrtc/flutter_webrtc.dart' as webrtc;
import 'package:get/get.dart';
import 'package:tflite_flutter/tflite_flutter.dart';
import 'package:image/image.dart' as img;
import '../core/utils/app_constants.dart';
import '../core/platform/frame_capture_platform.dart';
import 'storage_service.dart';
import 'mqtt_service_consolidated.dart';
import 'media_device_service.dart';
import 'webrtc_texture_bridge.dart';

/// Data structure for passing inference data to background processing
class InferenceData {
  final Object inputData;
  final int personClassId;
  final double confidenceThreshold;
  final Uint8List modelBytes;

  InferenceData({
    required this.inputData,
    required this.personClassId,
    required this.confidenceThreshold,
    required this.modelBytes,
  });
}

/// Detection box structure for debug visualization
class DetectionBox {
  final double x1, y1, x2, y2; // Normalized coordinates (0.0 - 1.0)
  final double confidence;
  final int classId;
  final String? className;

  DetectionBox({
    required this.x1,
    required this.y1,
    required this.x2,
    required this.y2,
    required this.confidence,
    required this.classId,
    this.className,
  });
}

/// Result structure for inference results
class InferenceResult {
  final double maxPersonConfidence;
  final int numDetections;
  final String? error;
  final List<DetectionBox> detectionBoxes; // Added for debug visualization
  InferenceResult({
    required this.maxPersonConfidence,
    required this.numDetections,
    this.error,
    this.detectionBoxes = const [],  });
}

/// Object categories for grouping detected objects
enum ObjectCategory {
  people,
  animals,
  vehicles,
  furniture,
  electronics,
  food,
  sports,
  kitchenware,
  accessories,
  other,
}

/// Helper function to get class name for COCO dataset class IDs
String _getClassNameForId(int classId) {
  // Complete COCO dataset class names (80 classes + background)
  const cocoClasses = {
    0: 'background',
    1: 'person',
    2: 'bicycle',
    3: 'car',
    4: 'motorcycle',
    5: 'airplane',
    6: 'bus',
    7: 'train',
    8: 'truck',
    9: 'boat',
    10: 'traffic light',
    11: 'fire hydrant',
    12: 'stop sign',
    13: 'parking meter',
    14: 'bench',
    15: 'bird',
    16: 'cat',
    17: 'dog',
    18: 'horse',
    19: 'sheep',
    20: 'cow',
    21: 'elephant',
    22: 'bear',
    23: 'zebra',
    24: 'giraffe',
    25: 'backpack',
    26: 'umbrella',
    27: 'handbag',
    28: 'tie',
    29: 'suitcase',
    30: 'frisbee',
    31: 'skis',
    32: 'snowboard',
    33: 'sports ball',
    34: 'kite',
    35: 'baseball bat',
    36: 'baseball glove',
    37: 'skateboard',
    38: 'surfboard',
    39: 'tennis racket',
    40: 'bottle',
    41: 'wine glass',
    42: 'cup',
    43: 'fork',
    44: 'knife',
    45: 'spoon',
    46: 'bowl',
    47: 'banana',
    48: 'apple',
    49: 'sandwich',
    50: 'orange',
    51: 'broccoli',
    52: 'carrot',
    53: 'hot dog',
    54: 'pizza',
    55: 'donut',
    56: 'cake',
    57: 'chair',
    58: 'couch',
    59: 'potted plant',
    60: 'bed',
    61: 'dining table',
    62: 'toilet',
    63: 'tv',
    64: 'laptop',
    65: 'mouse',
    66: 'remote',
    67: 'keyboard',
    68: 'cell phone',
    69: 'microwave',
    70: 'oven',
    71: 'toaster',
    72: 'sink',
    73: 'refrigerator',
    74: 'book',
    75: 'clock',
    76: 'vase',
    77: 'scissors',
    78: 'teddy bear',
    79: 'hair drier',
    80: 'toothbrush',
  };
  
  return cocoClasses[classId] ?? 'unknown';
}

/// Background inference function that runs in a separate isolate
Future<InferenceResult> _runInferenceInBackground(InferenceData data) async {
  try {
    // Load the interpreter from model bytes in the background isolate
    final interpreter = Interpreter.fromBuffer(data.modelBytes);
    
    // Get output tensor shapes to determine the correct structure
    final outputShapes = <List<int>>[];
    for (int i = 0; i < interpreter.getOutputTensors().length; i++) {
      outputShapes.add(interpreter.getOutputTensor(i).shape);
    }
    
    // For SSD MobileNet models, we typically have 4 outputs:
    // 0: detection_boxes [1, num_detections, 4] 
    // 1: detection_classes [1, num_detections]
    // 2: detection_scores [1, num_detections]  
    // 3: num_detections [1]
    
    final outputTensors = <int, Object>{};
    
    if (outputShapes.length >= 4) {
      // Standard SSD MobileNet format
      final numDetections = outputShapes[0][1]; // Get num_detections from boxes shape
      
      outputTensors[0] = List.generate(1, (_) => List.generate(numDetections, (_) => List.filled(4, 0.0))); // boxes
      outputTensors[1] = List.generate(1, (_) => List.filled(numDetections, 0.0)); // classes
      outputTensors[2] = List.generate(1, (_) => List.filled(numDetections, 0.0)); // scores
      outputTensors[3] = [0.0]; // num_detections
    } else {
      // Fallback for simpler models - use actual output shapes
      for (int i = 0; i < outputShapes.length; i++) {
        final shape = outputShapes[i];
        if (shape.length == 3 && shape[2] == 4) {
          // This looks like detection boxes [1, num_detections, 4]
          outputTensors[i] = List.generate(shape[0], (_) => List.generate(shape[1], (_) => List.filled(shape[2], 0.0)));
        } else if (shape.length == 2) {
          // This looks like scores or classes [1, num_detections]
          outputTensors[i] = List.generate(shape[0], (_) => List.filled(shape[1], 0.0));
        } else if (shape.length == 1) {
          // This looks like num_detections [1]
          outputTensors[i] = List.filled(shape[0], 0.0);
        }
      }
    }
      // Run inference (this heavy computation is now in background)
    interpreter.runForMultipleInputs([data.inputData], outputTensors);
    
    // Parse results for person detection
    double maxPersonConfidence = 0.0;
    int totalDetections = 0;
    List<DetectionBox> detectionBoxes = [];
    
    // Handle different output formats
    if (outputTensors.containsKey(2) && outputTensors.containsKey(1)) {
      // Standard SSD format with separate scores and classes
      final scores = outputTensors[2] as List<List<double>>;
      final classes = outputTensors[1] as List<List<double>>;
      final boxes = outputTensors[0] as List<List<List<double>>>;
      
      // Get number of detections
      if (outputTensors.containsKey(3)) {
        totalDetections = (outputTensors[3] as List<double>)[0].toInt();
      } else {
        totalDetections = scores[0].length;
      }
      
      for (int i = 0; i < totalDetections && i < scores[0].length; i++) {
        final classId = classes[0][i].toInt();
        final score = scores[0][i];
        
        // Extract bounding box coordinates (typically in format [y1, x1, y2, x2])
        final y1 = boxes[0][i][0];
        final x1 = boxes[0][i][1];
        final y2 = boxes[0][i][2];
        final x2 = boxes[0][i][3];
        
        // Create detection box for all valid detections
        if (score > 0.1) { // Low threshold for debug visualization
          detectionBoxes.add(DetectionBox(
            x1: x1,
            y1: y1,
            x2: x2,
            y2: y2,
            confidence: score,
            classId: classId,
            className: _getClassNameForId(classId),
          ));
        }
        
        if (classId == data.personClassId && score > maxPersonConfidence) {
          maxPersonConfidence = score;
        }
      }
    } else if (outputTensors.containsKey(0)) {
      // Simpler format - check if first output contains confidence scores
      final output = outputTensors[0];
      if (output is List<List<double>> && output.isNotEmpty) {
        // Assume first output contains confidence scores
        maxPersonConfidence = output[0].isNotEmpty ? output[0][0] : 0.0;
        totalDetections = 1;
      }
    }
    
    interpreter.close();
    
    return InferenceResult(
      maxPersonConfidence: maxPersonConfidence,
      numDetections: totalDetections,
      detectionBoxes: detectionBoxes,
    );
    
  } catch (e) {
    return InferenceResult(
      maxPersonConfidence: 0.0,
      numDetections: 0,
      error: e.toString(),
    );
  }
}

/// Service for person presence detection using TensorFlow Lite
class PersonDetectionService extends GetxService {
  // Dependencies
  final StorageService _storageService = Get.find<StorageService>();
  final WebRTCTextureBridge _textureBridge = Get.find<WebRTCTextureBridge>();
  
  // TensorFlow Lite interpreter
  Interpreter? _interpreter;
  Uint8List? _modelBytes; // Store model bytes for background processing
  // Observable properties
  final RxBool isEnabled = false.obs;
  final RxBool isPersonPresent = false.obs;
  final RxBool isProcessing = false.obs;
  final RxString lastError = ''.obs;
  final RxDouble confidence = 0.0.obs;
  final RxInt framesProcessed = 0.obs;
  
  // Multi-object detection properties
  final RxList<DetectionBox> detectedObjects = <DetectionBox>[].obs;
  final RxMap<String, int> objectCounts = <String, int>{}.obs;
  final RxMap<String, double> objectConfidences = <String, double>{}.obs;
  final RxBool anyObjectDetected = false.obs;
  
  // Debug visualization properties
  final RxBool isDebugVisualizationEnabled = false.obs;
  final RxList<DetectionBox> latestDetectionBoxes = <DetectionBox>[].obs;
  final RxnString debugVisualizationFrame = RxnString(); // Base64 encoded frame
    // Processing configuration for SSD MobileNet
  final int inputWidth = 300; // Matches your current model requirements
  final int inputHeight = 300; // Matches your current model requirements
  final int numChannels = 3;
  final double confidenceThreshold = 0.5; // Threshold for object detection
  final double objectDetectionThreshold = 0.3; // Lower threshold for general objects
  final int personClassId = 1; // Person class ID in COCO dataset

  // Frame processing timer and stream
  Timer? _processingTimer;
  webrtc.MediaStream? _cameraStream;
  webrtc.RTCVideoRenderer? _videoRenderer;
  
  // Platform support flags
  bool _isFrameCaptureSupported = false;
  int? _rendererTextureId;
  
  // Processing rate configuration - optimized for performance
  final Duration processingInterval = Duration(milliseconds: 2000); // Process 0.5 frames per second
  
  @override
  Future<void> onInit() async {
    super.onInit();
    
    // Check platform support for frame capture
    _isFrameCaptureSupported = await FrameCapturePlatform.isSupported();
    print('Frame capture platform support: $_isFrameCaptureSupported');
    
    // Load settings
    isEnabled.value = _storageService.read<bool>(AppConstants.keyPersonDetectionEnabled) ?? false;
    
    // Initialize if enabled
    if (isEnabled.value) {
      final modelInitialized = await _initializeModel();
      if (modelInitialized) {
        // Start detection immediately if model initialized successfully
        final detectionStarted = await startDetection();
        if (detectionStarted) {
          print('‚úÖ Person detection started successfully at startup');
        } else {
          print('‚ö†Ô∏è Person detection model initialized but failed to start camera');
        }
      }
    }
    
    // Listen for setting changes
    ever(isEnabled, (bool enabled) async {
      _storageService.write(AppConstants.keyPersonDetectionEnabled, enabled);
      if (enabled) {
        final modelInitialized = await _initializeModel();
        if (modelInitialized) {
          final detectionStarted = await startDetection();
          if (detectionStarted) {
            print('‚úÖ Person detection restarted via settings');
          } else {
            print('‚ö†Ô∏è Person detection model initialized but failed to start camera via settings');
          }
        }
      } else {
        _stopDetection();
      }
    });
  }
  
  @override
  void onClose() {
    _stopDetection();
    _interpreter?.close();
    super.onClose();
  }
    /// Initialize the TensorFlow Lite model
  Future<bool> _initializeModel() async {
    try {
      isProcessing.value = true;
      lastError.value = '';
      
      // Try to load the TensorFlow Lite model from assets
      try {
        // Load model as bytes for background processing
        final modelData = await rootBundle.load('assets/models/person_detect.tflite');
        _modelBytes = modelData.buffer.asUint8List();
        
        _interpreter = await Interpreter.fromAsset('assets/models/person_detect.tflite');
        
        // Verify model input/output shape
        final inputShape = _interpreter!.getInputTensor(0).shape;
        final outputShape = _interpreter!.getOutputTensor(0).shape;
        
        print('Person detection model loaded successfully');
        print('Input shape: $inputShape');
        print('Output shape: $outputShape');
        
        return true;
        
      } catch (e) {
        print('Failed to load TensorFlow Lite model: $e');
        
        // Check if this is a native library issue
        if (e.toString().contains('libtensorflowlite_c') || 
            e.toString().contains('Failed to load dynamic library')) {
          lastError.value = 'TensorFlow Lite native libraries not available. Person detection will use fallback mode.';
          print('TensorFlow Lite native libraries missing. This is common on Windows.');
          print('Person detection will continue with simulated detection for development.');
          
          // Enable fallback mode - simulate model for development/testing
          _interpreter = null; // Mark as unavailable but don't fail
          return true; // Return true to allow the service to continue
          
        } else {
          // Re-throw other errors (model file issues, etc.)
          throw e;
        }
      }
      
    } catch (e) {
      lastError.value = 'Failed to initialize person detection: $e';
      print('Error initializing person detection: $e');
      return false;
    } finally {
      isProcessing.value = false;
    }
  }
  
  /// Start person detection with camera access
  Future<bool> startDetection({String? deviceId}) async {
    if (!isEnabled.value) {
      lastError.value = 'Person detection is disabled';
      return false;
    }

    // Initialize model if not already done
    if (_interpreter == null) {
      final initialized = await _initializeModel();
      if (!initialized) return false;
    }

    try {
      // Get device ID from MediaDeviceService if not provided
      String? actualDeviceId = deviceId;
      if (actualDeviceId == null || actualDeviceId.isEmpty) {
        try {
          final mediaDeviceService = Get.find<MediaDeviceService>();
          if (mediaDeviceService.selectedVideoInput.value != null) {
            actualDeviceId = mediaDeviceService.selectedVideoInput.value!.deviceId;
            print('üë§ Using camera from MediaDeviceService: ${mediaDeviceService.selectedVideoInput.value!.label}');
          } else if (mediaDeviceService.videoInputs.isNotEmpty) {
            actualDeviceId = mediaDeviceService.videoInputs.first.deviceId;
            print('üë§ Using first available camera: ${mediaDeviceService.videoInputs.first.label}');
          }
        } catch (e) {
          print('‚ö†Ô∏è MediaDeviceService not available, will use default camera: $e');
        }
      }

      // Initialize video renderer if not already done
      if (_videoRenderer == null) {
        _videoRenderer = webrtc.RTCVideoRenderer();
        await _videoRenderer!.initialize();
      }
      
      // Configure camera constraints for efficient processing
      final Map<String, dynamic> mediaConstraints = {
        'audio': false,
        'video': {
          'width': {'ideal': 320}, // Lower resolution for better performance
          'height': {'ideal': 240}, // Lower resolution for better performance
          'frameRate': {'ideal': 10}, // Even lower frame rate for efficiency
          'facingMode': 'user', // Default to front camera
        }
      };

      // Add device ID if available
      if (actualDeviceId != null && actualDeviceId.isNotEmpty) {
        mediaConstraints['video']['deviceId'] = actualDeviceId;
        print('üë§ Person detection using camera device: $actualDeviceId');
      } else {
        print('‚ö†Ô∏è No specific camera device selected, using default');
      }      // Get camera stream
      _cameraStream = await webrtc.navigator.mediaDevices.getUserMedia(mediaConstraints);
      _videoRenderer!.srcObject = _cameraStream;

      // Register renderer with WebRTC texture bridge for proper texture access
      if (_isFrameCaptureSupported) {
        try {
          // Register the video renderer with the texture bridge
          _rendererTextureId = _textureBridge.registerRenderer(_videoRenderer!);
          print('‚úÖ Registered WebRTC renderer with texture bridge: $_rendererTextureId');
          
          // Get the actual WebRTC texture ID for verification
          final webrtcTextureId = _textureBridge.getWebRTCTextureId(_rendererTextureId!);
          print('üîó WebRTC texture ID: $webrtcTextureId');
          
          // Attempt to get native platform texture handle for enhanced access
          final platformTextureId = await _textureBridge.getNativePlatformTextureId(_rendererTextureId!);
          if (platformTextureId != null) {
            print('üèÜ Got native platform texture ID: $platformTextureId for enhanced frame capture');
          } else {
            print('‚ö†Ô∏è Native platform texture access not available, using standard WebRTC capture');
          }
          
        } catch (e) {
          print('‚ùå Error registering with texture bridge: $e, falling back to standard texture ID');
          _rendererTextureId = _videoRenderer!.textureId;
        }
      }

      // Start frame processing
      _startFrameProcessing();

      print('Person detection started successfully with texture ID: $_rendererTextureId');
      return true;

    } catch (e) {
      lastError.value = 'Failed to start camera: $e';
      print('Error starting person detection: $e');
      return false;
    }
  }
  
  /// Stop person detection
  void _stopDetection() {
    _processingTimer?.cancel();
    _processingTimer = null;
    
    // Stop camera stream
    _cameraStream?.getTracks().forEach((track) => track.stop());
    _cameraStream?.dispose();
    _cameraStream = null;
    
    // Dispose video renderer
    _videoRenderer?.dispose();
    _videoRenderer = null;
    
    // Reset texture ID
    _rendererTextureId = null;
    
    // Reset state
    isPersonPresent.value = false;
    confidence.value = 0.0;
    isProcessing.value = false;
    
    print('Person detection stopped');
  }
  
  /// Start periodic frame processing
  void _startFrameProcessing() {
    _processingTimer?.cancel();
    _processingTimer = Timer.periodic(processingInterval, (_) {
      _processCurrentFrame();
    });
  }
  
  /// Process the current camera frame for person detection
  Future<void> _processCurrentFrame() async {
    if (_videoRenderer == null || isProcessing.value) {
      return;
    }
    
    try {
      isProcessing.value = true;
      
      // If TensorFlow Lite interpreter is available, use real detection
      if (_interpreter != null) {        // Capture frame from video renderer
        final frameData = await _captureFrame();
        if (frameData == null) {
          if (framesProcessed.value % 20 == 0) {
            print('‚ö†Ô∏è  Frame capture failed - no frame data available (frame ${framesProcessed.value})');
          }
          
          // If debug visualization is enabled and we can't capture real frames, use realistic test data
          if (isDebugVisualizationEnabled.value) {
            if (framesProcessed.value % 2 == 0) { // Generate more frequently for smooth visualization
              _generateRealisticDebugData();
              if (framesProcessed.value % 20 == 0) {
                print('üé® Generated realistic test data for debug visualization (frame ${framesProcessed.value})');
              }
            }
          }
          framesProcessed.value++; // Still count the frame
          return;
        }
        
        // Determine if this is real or synthetic frame data
        final isRealFrame = _isFrameCaptureSupported && _rendererTextureId != null && _rendererTextureId! > 0;
        
        // Log successful frame capture occasionally
        if (framesProcessed.value % 50 == 0) {
          final frameType = isRealFrame ? "real WebRTC" : "test";
          print('üì∑ Frame captured successfully ($frameType): ${frameData.length} bytes (${inputWidth}x${inputHeight})');
        }
          // Preprocess frame for model input
        final inputData = _preprocessFrame(frameData);
        
        // Check if model bytes are available for background processing
        if (_modelBytes == null) {
          print('‚ö†Ô∏è Model bytes not available for background processing');
          return;
        }
        
        try {
          // Run inference in background using compute to prevent UI blocking
          final inferenceData = InferenceData(
            inputData: inputData,
            personClassId: personClassId,
            confidenceThreshold: confidenceThreshold,
            modelBytes: _modelBytes!,
          );
          
          final result = await compute(_runInferenceInBackground, inferenceData);
          
          if (result.error != null) {
            throw Exception('Background inference error: ${result.error}');
          }          confidence.value = result.maxPersonConfidence;
          
          // Process all detected objects
          _processAllDetectedObjects(result.detectionBoxes);
          
          // Store debug visualization data if enabled
          if (isDebugVisualizationEnabled.value) {
            latestDetectionBoxes.value = result.detectionBoxes;
            
            // Store the frame data for debug visualization (convert raw RGBA to PNG)
            try {
              final debugFrame = _convertRawFrameToPng(frameData);
              if (debugFrame != null) {
                debugVisualizationFrame.value = base64Encode(debugFrame);
              } else {
                print('‚ö†Ô∏è Failed to convert frame to PNG for debug visualization');
              }
            } catch (e) {
              print('‚ö†Ô∏è Failed to encode frame for debug visualization: $e');
            }
          }
          
          // Enhanced debugging output
          if (framesProcessed.value % 10 == 0) { // Log every 10th frame
            print('ü§ñ Person Detection Frame ${framesProcessed.value}:');
            print('   üìä Detections found: ${result.numDetections}');
            print('   üéØ Max person confidence: ${result.maxPersonConfidence.toStringAsFixed(3)}');
            print('   üë§ Person present: ${result.maxPersonConfidence > confidenceThreshold} (threshold: $confidenceThreshold)');
            print('   ‚ö° Processed in background isolate');
            if (isDebugVisualizationEnabled.value) {
              print('   üêõ Debug boxes stored: ${result.detectionBoxes.length}');
            }
          }
          
        } catch (e) {
          print('Error running background inference: $e');
          // Fallback to simple single output processing if multi-output fails
          final outputShape = _interpreter!.getOutputTensor(0).shape;
          
          // Create output tensor with proper shape [batch_size, output_size]
          final outputSize = outputShape.isNotEmpty ? outputShape.last : 1;
          final output = List.generate(1, (_) => List.filled(outputSize, 0.0));
          
          _interpreter!.run(inputData, output);
          
          // For simple models, assume first output is confidence
          confidence.value = output[0][0];
        }        // Update presence detection
        final wasPersonPresent = isPersonPresent.value;
        isPersonPresent.value = confidence.value > confidenceThreshold;
        
        // Publish to MQTT if status changed or periodically for all objects
        if (wasPersonPresent != isPersonPresent.value || framesProcessed.value % 20 == 0) {
          _publishAllDetections();
          if (wasPersonPresent != isPersonPresent.value) {
            print('üö® Person presence changed: ${isPersonPresent.value ? "DETECTED" : "NOT DETECTED"} (confidence: ${confidence.value.toStringAsFixed(3)})');
          }
        }
        
        framesProcessed.value++;
        
      } else {
        // Fallback mode: Simulate person detection for development/testing
        // This provides basic functionality when TensorFlow Lite is not available
        _simulatePersonDetection();
      }
      
    } catch (e) {
      lastError.value = 'Frame processing error: $e';
      print('Error processing frame: $e');
    } finally {
      isProcessing.value = false;
    }
  }
  
  /// Simulate person detection when TensorFlow Lite is not available
  void _simulatePersonDetection() {
    // Simple simulation: randomly detect person presence
    // In a real implementation, this could use alternative detection methods
    final random = DateTime.now().millisecondsSinceEpoch % 1000;
    final simulatedConfidence = (random / 1000.0) * 0.3 + 0.4; // Between 0.4 and 0.7
    
    confidence.value = simulatedConfidence;
    
    // Update presence detection with some randomness
    final wasPersonPresent = isPersonPresent.value;
    isPersonPresent.value = simulatedConfidence > confidenceThreshold;
      // Add some stability - don't change state too frequently
    if (framesProcessed.value % 10 == 0) {
      // Publish to MQTT if status changed
      if (wasPersonPresent != isPersonPresent.value) {
        _publishAllDetections();
      }
    }
      framesProcessed.value++;
    
    // Generate synthetic debug data if visualization is enabled
    if (isDebugVisualizationEnabled.value && framesProcessed.value % 5 == 0) {
      _generateSyntheticDebugData();
    }
    
    // Log simulation mode periodically
    if (framesProcessed.value % 20 == 0) {
      print('Person detection running in simulation mode (frame ${framesProcessed.value})');
    }
  }  /// Capture current frame from video renderer
  Future<Uint8List?> _captureFrame() async {
    try {
      // Check if platform capture is supported and texture ID is available
      if (_isFrameCaptureSupported && _rendererTextureId != null && _rendererTextureId! > 0) {
        // Use WebRTC texture bridge for enhanced frame capture
        final frameData = await _textureBridge.captureFrame(
          _rendererTextureId!,
          inputWidth,
          inputHeight,
        );
        
        if (frameData != null && frameData.isNotEmpty) {
          // Validate frame data size
          final expectedSize = inputWidth * inputHeight * 4; // RGBA
          if (frameData.length == expectedSize) {
            // Log successful capture for debugging (but not too frequently)
            if (framesProcessed.value % 100 == 0) {
              print('‚úÖ Real frame captured via WebRTC bridge: ${frameData.length} bytes');
            }
            return frameData;
          } else {
            print('Warning: Frame data size mismatch. Expected: $expectedSize, Got: ${frameData.length}');
            // Try to process anyway if size is close
            if (frameData.length >= expectedSize * 0.8) {
              return frameData;
            }
          }
        } else {
          if (framesProcessed.value % 50 == 0) {
            print('‚ö†Ô∏è WebRTC texture bridge capture returned null (frame ${framesProcessed.value})');
          }
        }
      } else {
        if (framesProcessed.value % 100 == 0) {
          print('‚ö†Ô∏è Frame capture not available - platform support: $_isFrameCaptureSupported, texture ID: $_rendererTextureId (frame ${framesProcessed.value})');
        }
      }
      
      // Enhanced fallback: Try alternative WebRTC frame access methods
      if (_videoRenderer != null && _cameraStream != null) {
        // Future implementation could include:
        // 1. Direct WebRTC frame callback registration
        // 2. Canvas-based capture for web platforms
        // 3. Platform-specific video pipeline access
        
        // For now, attempt to create a realistic test frame based on camera state
        return _createRealisticTestFrame();
      }
      
      return null;
      
    } catch (e) {
      print('Error capturing frame: $e');
      lastError.value = 'Frame capture error: $e';
      return null;
    }
  }

  /// Create a realistic test frame that simulates camera data
  /// This provides better debugging experience until real WebRTC capture is implemented
  Uint8List? _createRealisticTestFrame() {
    try {
      final frameData = Uint8List(inputWidth * inputHeight * 4);
      
      // Create a more realistic camera-like frame with:
      // - Natural color variations
      // - Simulated person-like shapes
      // - Temporal changes to simulate video
      
      final frameTime = DateTime.now().millisecondsSinceEpoch / 1000.0;
      
      for (int y = 0; y < inputHeight; y++) {
        for (int x = 0; x < inputWidth; x++) {
          final offset = (y * inputWidth + x) * 4;
          
          // Create a realistic background with subtle noise
          final baseR = 120 + (math.sin(x * 0.02) * 20).toInt();
          final baseG = 140 + (math.cos(y * 0.03) * 15).toInt();
          final baseB = 160 + (math.sin((x + y) * 0.01) * 10).toInt();
          
          // Add temporal variation to simulate live video
          final timeOffset = (math.sin(frameTime * 0.5) * 10).toInt();
          
          // Simulate a person-like shape in the center area
          final centerX = inputWidth / 2;
          final centerY = inputHeight / 2;
          final distFromCenter = math.sqrt(math.pow(x - centerX, 2) + math.pow(y - centerY, 2));
          
          if (distFromCenter < 80 && y > inputHeight * 0.3 && y < inputHeight * 0.8) {
            // Person-like region (darker, skin-tone colors)
            frameData[offset + 0] = (200 + timeOffset).clamp(0, 255); // R
            frameData[offset + 1] = (170 + timeOffset).clamp(0, 255); // G
            frameData[offset + 2] = (140 + timeOffset).clamp(0, 255); // B
          } else {
            // Background
            frameData[offset + 0] = (baseR + timeOffset).clamp(0, 255); // R
            frameData[offset + 1] = (baseG + timeOffset).clamp(0, 255); // G
            frameData[offset + 2] = (baseB + timeOffset).clamp(0, 255); // B
          }
          
          frameData[offset + 3] = 255; // A (fully opaque)
        }
      }
      
      return frameData;
      
    } catch (e) {
      print('Error creating test frame: $e');
      return null;
    }
  }
  
  /// Preprocess frame data for model input using the image package
  Object _preprocessFrame(Uint8List frameData) {
    try {
      img.Image? image;
      
      // Check if this is RGBA data from platform capture or encoded image
      if (frameData.length == inputWidth * inputHeight * 4) {
        // Raw RGBA data from platform capture
        image = img.Image.fromBytes(
          width: inputWidth,
          height: inputHeight,
          bytes: frameData.buffer,
          format: img.Format.uint8,
          numChannels: 4,
        );
      } else {
        // Encoded image data (JPEG, PNG, etc.)
        image = img.decodeImage(frameData);
      }
      
      if (image == null) {
        throw Exception('Failed to decode image');
      }
      
      // Resize image to model input size if needed
      if (image.width != inputWidth || image.height != inputHeight) {
        image = img.copyResize(image, width: inputWidth, height: inputHeight);
      }
      
      // Check model input tensor type to determine preprocessing
      final inputTensor = _interpreter!.getInputTensor(0);
      final inputType = inputTensor.type;
      
      // Check if this is a quantized model (uint8) or float model
      // TfLiteType enum values vary by package version, so we check the string representation
      final isQuantized = inputType.toString().contains('uint8') || 
                         inputType.toString().contains('UINT8');
      
      if (isQuantized) {
        // Quantized model - use efficient Uint8List for raw uint8 values (0-255)
        // Create flat tensor data: [batch_size * height * width * channels]
        final totalSize = 1 * inputHeight * inputWidth * numChannels;
        final input = Uint8List(totalSize);
        
        int index = 0;
        for (int y = 0; y < inputHeight; y++) {
          for (int x = 0; x < inputWidth; x++) {
            final pixel = image.getPixel(x, y);
            
            // Store raw RGB values (0-255) for quantized model in BHWC format
            input[index++] = pixel.r.toInt().clamp(0, 255);
            input[index++] = pixel.g.toInt().clamp(0, 255);
            input[index++] = pixel.b.toInt().clamp(0, 255);
          }
        }
        
        // Reshape to 4D tensor format [1, height, width, channels]
        return input.reshape([1, inputHeight, inputWidth, numChannels]);
        
      } else {
        // Float model - use efficient Float32List and normalize to [0, 1] range
        // Create flat tensor data: [batch_size * height * width * channels]
        final totalSize = 1 * inputHeight * inputWidth * numChannels;
        final input = Float32List(totalSize);
        
        int index = 0;
        for (int y = 0; y < inputHeight; y++) {
          for (int x = 0; x < inputWidth; x++) {
            final pixel = image.getPixel(x, y);
            
            // Extract RGB values and normalize to [0, 1] range in BHWC format
            input[index++] = (pixel.r / 255.0).clamp(0.0, 1.0);
            input[index++] = (pixel.g / 255.0).clamp(0.0, 1.0);
            input[index++] = (pixel.b / 255.0).clamp(0.0, 1.0);
          }
        }
        
        // Reshape to 4D tensor format [1, height, width, channels]  
        return input.reshape([1, inputHeight, inputWidth, numChannels]);
      }
    } catch (e) {
      print('Error preprocessing frame: $e');
      
      // Return appropriate fallback based on model type
      try {
        final inputTensor = _interpreter!.getInputTensor(0);
        final inputType = inputTensor.type;
        
        // Check if this is a quantized model using string representation
        final isQuantized = inputType.toString().contains('uint8') || 
                           inputType.toString().contains('UINT8');
        
        if (isQuantized) {
          // Return efficient Uint8List filled with zeros for quantized model
          final totalSize = 1 * inputHeight * inputWidth * numChannels;
          final input = Uint8List(totalSize);
          return input.reshape([1, inputHeight, inputWidth, numChannels]);
        } else {
          // Return efficient Float32List filled with zeros for float model
          final totalSize = 1 * inputHeight * inputWidth * numChannels;
          final input = Float32List(totalSize);
          return input.reshape([1, inputHeight, inputWidth, numChannels]);
        }
        
      } catch (_) {
        // If we can't determine type, default to efficient Float32List 4D tensor
        final totalSize = 1 * inputHeight * inputWidth * numChannels;
        final input = Float32List(totalSize);
        return input.reshape([1, inputHeight, inputWidth, numChannels]);
      }
    }
  }
  
  /// Converts raw RGBA frame data to PNG bytes for debug visualization
  Uint8List? _convertRawFrameToPng(Uint8List rawRgbaData) {
    try {
      // Expected frame size is 300x300x4 bytes (RGBA)
      const int width = 300;
      const int height = 300;
      const int channels = 4; // RGBA
      
      if (rawRgbaData.length != width * height * channels) {
        print('‚ö†Ô∏è Unexpected frame data size: ${rawRgbaData.length} bytes (expected ${width * height * channels})');
        return null;
      }
      
      // Create an image from raw RGBA data
      final image = img.Image(width: width, height: height);
      
      // Copy RGBA data to image
      for (int y = 0; y < height; y++) {
        for (int x = 0; x < width; x++) {
          final int pixelIndex = (y * width + x) * channels;
          final int r = rawRgbaData[pixelIndex];
          final int g = rawRgbaData[pixelIndex + 1];
          final int b = rawRgbaData[pixelIndex + 2];
          final int a = rawRgbaData[pixelIndex + 3];
          
          image.setPixelRgba(x, y, r, g, b, a);
        }
      }
      
      // Encode as PNG
      return Uint8List.fromList(img.encodePng(image));
    } catch (e) {
      print('‚ö†Ô∏è Error converting raw frame to PNG: $e');
      return null;
    }  }

  /// Process all detected objects and update observable properties
  void _processAllDetectedObjects(List<DetectionBox> detectionBoxes) {
    // Filter objects above the general detection threshold
    final validDetections = detectionBoxes
        .where((box) => box.confidence > objectDetectionThreshold)
        .toList();
    
    // Update detected objects list
    detectedObjects.value = validDetections;
    
    // Update object counts and confidences
    final counts = <String, int>{};
    final confidences = <String, double>{};
    
    for (final detection in validDetections) {
      final className = detection.className ?? 'unknown';
      counts[className] = (counts[className] ?? 0) + 1;
      
      // Keep the highest confidence for each class
      if (!confidences.containsKey(className) || 
          detection.confidence > confidences[className]!) {
        confidences[className] = detection.confidence;
      }
    }
    
    objectCounts.value = counts;
    objectConfidences.value = confidences;
    anyObjectDetected.value = validDetections.isNotEmpty;
    
    // Log detected objects periodically
    if (framesProcessed.value % 10 == 0 && validDetections.isNotEmpty) {
      print('üîç Objects detected:');
      for (final entry in counts.entries) {
        final confidence = confidences[entry.key]?.toStringAsFixed(2) ?? '0.00';
        print('   ${entry.key}: ${entry.value} (max confidence: $confidence)');
      }
    }
  }

  /// Enhanced MQTT publishing for all detected objects
  void _publishAllDetections() {
    try {
      if (Get.isRegistered<MqttService>()) {
        final mqttService = Get.find<MqttService>();
          // Publish comprehensive detection data
        final detectionData = {
          'timestamp': DateTime.now().toIso8601String(),
          'frames_processed': framesProcessed.value,
          'person_present': isPersonPresent.value,
          'person_confidence': confidence.value,
          'any_object_detected': anyObjectDetected.value,
          'total_objects': detectedObjects.length,
          'object_counts': Map<String, int>.from(objectCounts),
          'object_confidences': Map<String, double>.from(objectConfidences),
          'detected_objects': detectedObjects.map((box) => {
            'class_name': box.className,
            'class_id': box.classId,
            'confidence': box.confidence,
            'bounding_box': {
              'x1': box.x1,
              'y1': box.y1,
              'x2': box.x2,
              'y2': box.y2,
            },
          }).toList(),
        };
        
        // Publish to general object detection topic
        mqttService.publishJsonToTopic(
          'kingkiosk/${mqttService.deviceName.value}/object_detection', 
          detectionData
        );
        
        // Also publish to legacy person presence topic for backward compatibility
        final presenceData = {
          'person_present': isPersonPresent.value,
          'confidence': confidence.value,
          'timestamp': DateTime.now().toIso8601String(),
          'frames_processed': framesProcessed.value,
        };
        
        mqttService.publishJsonToTopic(
          'kingkiosk/${mqttService.deviceName.value}/person_presence', 
          presenceData
        );
        
        print('üì° Published object detection data: ${objectCounts.length} object types detected');
      }
    } catch (e) {
      print('Error publishing detection data to MQTT: $e');
    }  }
  
  /// Toggle person detection enabled/disabled
  void toggleEnabled() {
    isEnabled.value = !isEnabled.value;
  }
    /// Get current detection status
  Map<String, dynamic> getStatus() {
    return {
      'enabled': isEnabled.value,
      'person_present': isPersonPresent.value,
      'confidence': confidence.value,
      'processing': isProcessing.value,
      'frames_processed': framesProcessed.value,
      'last_error': lastError.value,
      // Multi-object detection status
      'any_object_detected': anyObjectDetected.value,
      'total_objects': detectedObjects.length,
      'object_counts': Map<String, int>.from(objectCounts),
      'object_confidences': Map<String, double>.from(objectConfidences),
      'detected_objects': detectedObjects.map((box) => {
        'class_name': box.className,
        'class_id': box.classId,
        'confidence': box.confidence,
      }).toList(),
    };
  }
  
  /// Check if camera is available for detection
  Future<bool> isCameraAvailable() async {
    try {
      final devices = await webrtc.navigator.mediaDevices.enumerateDevices();
      final videoDevices = devices.where((device) => device.kind == 'videoinput').toList();
      return videoDevices.isNotEmpty;
    } catch (e) {
      print('Error checking camera availability: $e');
      return false;
    }
  }
  
  /// Get the currently selected camera device from MediaDeviceService
  String? getSelectedCameraDevice() {
    try {
      final mediaDeviceService = Get.find<MediaDeviceService>();
      if (mediaDeviceService.selectedVideoInput.value != null) {
        return mediaDeviceService.selectedVideoInput.value!.deviceId;
      } else if (mediaDeviceService.videoInputs.isNotEmpty) {
        return mediaDeviceService.videoInputs.first.deviceId;
      }
    } catch (e) {
      print('‚ö†Ô∏è MediaDeviceService not available for camera selection: $e');
    }
    return null;
  }

  /// Restart detection with a new camera device
  Future<bool> switchCamera(String deviceId) async {
    if (!isEnabled.value) {
      print('‚ö†Ô∏è Person detection is disabled, cannot switch camera');
      return false;
    }

    // Stop current detection
    _stopDetection();
    
    // Wait a moment for cleanup
    await Future.delayed(Duration(milliseconds: 100));
    
    // Start with new device
    return await startDetection(deviceId: deviceId);
  }
  /// Get available camera devices from MediaDeviceService
  List<String> getAvailableCameras() {
    try {
      final mediaDeviceService = Get.find<MediaDeviceService>();
      return mediaDeviceService.videoInputs.map((device) => device.deviceId).toList();
    } catch (e) {
      print('‚ö†Ô∏è MediaDeviceService not available for camera enumeration: $e');
      return [];    }
  }

  // Object Detection Query Methods
  
  /// Check if a specific object type is detected
  bool isObjectDetected(String objectName) {
    return objectCounts.containsKey(objectName) && objectCounts[objectName]! > 0;
  }
  
  /// Get the count of a specific object type
  int getObjectCount(String objectName) {
    return objectCounts[objectName] ?? 0;
  }
  
  /// Get the confidence of a specific object type
  double getObjectConfidence(String objectName) {
    return objectConfidences[objectName] ?? 0.0;
  }
  
  /// Get all detected objects of a specific type
  List<DetectionBox> getObjectsOfType(String objectName) {
    return detectedObjects.where((box) => box.className == objectName).toList();
  }
  
  /// Get detected objects by category
  List<DetectionBox> getObjectsByCategory(ObjectCategory category) {
    final categoryObjects = _getObjectNamesForCategory(category);
    return detectedObjects.where((box) => 
      box.className != null && categoryObjects.contains(box.className!)
    ).toList();
  }
  
  /// Check if any object from a category is detected
  bool isCategoryDetected(ObjectCategory category) {
    return getObjectsByCategory(category).isNotEmpty;
  }
  
  /// Get all detected animal objects
  List<DetectionBox> getDetectedAnimals() {
    return getObjectsByCategory(ObjectCategory.animals);
  }
  
  /// Get all detected vehicles
  List<DetectionBox> getDetectedVehicles() {
    return getObjectsByCategory(ObjectCategory.vehicles);
  }
  
  /// Get all detected furniture objects
  List<DetectionBox> getDetectedFurniture() {
    return getObjectsByCategory(ObjectCategory.furniture);
  }
  
  /// Get all detected food items
  List<DetectionBox> getDetectedFood() {
    return getObjectsByCategory(ObjectCategory.food);
  }
  
  /// Get object names for a specific category
  List<String> _getObjectNamesForCategory(ObjectCategory category) {
    switch (category) {
      case ObjectCategory.people:
        return ['person'];
      case ObjectCategory.animals:
        return ['bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe'];
      case ObjectCategory.vehicles:
        return ['bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat'];
      case ObjectCategory.furniture:
        return ['chair', 'couch', 'bed', 'dining table', 'toilet'];
      case ObjectCategory.electronics:
        return ['tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'refrigerator'];
      case ObjectCategory.food:
        return ['banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake'];
      case ObjectCategory.sports:
        return ['frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket'];
      case ObjectCategory.kitchenware:
        return ['bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl'];
      case ObjectCategory.accessories:
        return ['backpack', 'umbrella', 'handbag', 'tie', 'suitcase'];
      case ObjectCategory.other:
        return ['traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'potted plant', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'];
    }
  }

  // Debug Visualization Methods
  
  /// Enable debug visualization to show detection boxes
  void enableDebugVisualization() {
    isDebugVisualizationEnabled.value = true;
    print('üêõ Debug visualization enabled - detection boxes will be captured');
    
    // Immediately generate synthetic data for testing
    _generateSyntheticDebugData();
    print('üé® Generated initial synthetic data for debug visualization');
  }

  /// Disable debug visualization
  void disableDebugVisualization() {
    isDebugVisualizationEnabled.value = false;
    latestDetectionBoxes.clear();
    debugVisualizationFrame.value = null;
    print('üêõ Debug visualization disabled');
  }

  /// Toggle debug visualization
  void toggleDebugVisualization() {
    if (isDebugVisualizationEnabled.value) {
      disableDebugVisualization();
    } else {
      enableDebugVisualization();
    }
  }

  /// Get the latest detection boxes for debug visualization
  List<DetectionBox> getLatestDetectionBoxes() {
    return List.from(latestDetectionBoxes);
  }

  /// Get the latest frame as base64 for debug visualization
  String? getLatestDebugFrame() {
    return debugVisualizationFrame.value;
  }  /// Generate realistic debug data with simulated camera frames and detection boxes
  void _generateRealisticDebugData() {
    if (!isDebugVisualizationEnabled.value) return;
    
    try {
      // Create realistic detection boxes that change over time
      final frameTime = DateTime.now().millisecondsSinceEpoch / 1000.0;
      final boxes = <DetectionBox>[];
      
      // Simulate a person moving slightly in the center
      final personX = 0.4 + 0.1 * math.sin(frameTime * 0.3);
      final personY = 0.3 + 0.05 * math.cos(frameTime * 0.4);
      
      boxes.add(DetectionBox(
        x1: personX, 
        y1: personY, 
        x2: personX + 0.2, 
        y2: personY + 0.4,
        confidence: 0.85 + 0.1 * math.sin(frameTime * 0.8),
        classId: 1,
        className: 'person',
      ));
      
      // Occasionally add other objects
      if ((frameTime * 2).toInt() % 3 == 0) {
        boxes.add(DetectionBox(
          x1: 0.1, y1: 0.1, x2: 0.25, y2: 0.3,
          confidence: 0.45 + 0.05 * math.cos(frameTime),
          classId: 57,
          className: 'chair',
        ));
      }
      
      // Update detection data
      latestDetectionBoxes.value = boxes;
      confidence.value = boxes.isNotEmpty ? boxes.first.confidence : 0.0;
      isPersonPresent.value = boxes.any((box) => box.classId == 1 && box.confidence > confidenceThreshold);
      
      // Generate a realistic test frame
      final testFrame = _createRealisticTestFrame();
      if (testFrame != null) {
        // Convert to PNG for debug visualization
        final debugFrame = _convertRawFrameToPng(testFrame);
        if (debugFrame != null) {
          debugVisualizationFrame.value = base64Encode(debugFrame);
        }
      }
      
      if (framesProcessed.value % 50 == 0) {
        print('üé® Generated realistic debug data with ${boxes.length} detection boxes (person detected: ${isPersonPresent.value})');
      }
    } catch (e) {
      print('‚ö†Ô∏è Failed to generate realistic debug data: $e');
      // Fallback to basic synthetic data
      _generateSyntheticDebugData();
    }
  }

  /// Generate basic synthetic debug data for testing
  void _generateSyntheticDebugData() {
    if (!isDebugVisualizationEnabled.value) return;
    
    // Create synthetic detection boxes for testing
    final syntheticBoxes = <DetectionBox>[
      DetectionBox(
        x1: 0.3, y1: 0.2, x2: 0.7, y2: 0.8, // Person in center
        confidence: 0.85,
        classId: 1,
        className: 'person',
      ),
      DetectionBox(
        x1: 0.1, y1: 0.1, x2: 0.25, y2: 0.3, // Object in top-left
        confidence: 0.45,
        classId: 3,
        className: 'car',
      ),
      DetectionBox(
        x1: 0.75, y1: 0.6, x2: 0.9, y2: 0.9, // Object in bottom-right
        confidence: 0.62,
        classId: 2,
        className: 'bicycle',
      ),
    ];
    
    // Update detection data
    latestDetectionBoxes.value = syntheticBoxes;
    confidence.value = 0.85; // High confidence for person
    isPersonPresent.value = true;
    
    // Generate a simple synthetic image as base64
    try {
      // Create a simple colored rectangle as a placeholder
      // This represents a 300x300 RGB image (red background for testing)
      final width = 300;
      final height = 300;
      final imageData = Uint8List(width * height * 3); // RGB format
      
      // Fill with a gradient pattern for visual testing
      for (int y = 0; y < height; y++) {
        for (int x = 0; x < width; x++) {
          final index = (y * width + x) * 3;
          imageData[index] = (x / width * 255).toInt(); // Red gradient
          imageData[index + 1] = (y / height * 255).toInt(); // Green gradient  
          imageData[index + 2] = 128; // Blue constant
        }
      }
      
      // Convert to PNG format using the image package
      final image = img.Image.fromBytes(
        width: width,
        height: height,
        bytes: imageData.buffer,
        format: img.Format.uint8,
        numChannels: 3,
      );
      
      final pngBytes = img.encodePng(image);
      debugVisualizationFrame.value = base64Encode(pngBytes);
      
      print('üé® Generated synthetic debug visualization data with ${syntheticBoxes.length} detection boxes');
      
    } catch (e) {
      print('‚ö†Ô∏è Failed to generate synthetic debug data: $e');
      // Fallback: just update detection boxes without frame
      debugVisualizationFrame.value = null;
    }
  }
}
